{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a name='1'></a>\n#### 1. Install all the required libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl","metadata":{"execution":{"iopub.status.busy":"2025-03-26T03:38:23.185353Z","iopub.execute_input":"2025-03-26T03:38:23.185713Z","iopub.status.idle":"2025-03-26T03:38:46.379096Z","shell.execute_reply.started":"2025-03-26T03:38:23.185689Z","shell.execute_reply":"2025-03-26T03:38:46.378274Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.8/410.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.7/335.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\n# disable Weights and Biases\nos.environ['WANDB_DISABLED']=\"true\"","metadata":{"execution":{"iopub.status.busy":"2025-03-26T03:38:49.250146Z","iopub.execute_input":"2025-03-26T03:38:49.250468Z","iopub.status.idle":"2025-03-26T03:38:49.254339Z","shell.execute_reply.started":"2025-03-26T03:38:49.250442Z","shell.execute_reply":"2025-03-26T03:38:49.253507Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig\n)\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np\nfrom huggingface_hub import interpreter_login\n\ninterpreter_login()","metadata":{"execution":{"iopub.status.busy":"2025-03-26T03:38:50.986904Z","iopub.execute_input":"2025-03-26T03:38:50.987249Z","iopub.status.idle":"2025-03-26T03:39:37.749109Z","shell.execute_reply.started":"2025-03-26T03:38:50.987222Z","shell.execute_reply":"2025-03-26T03:39:37.748306Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your token (input will not be visible):  ········\nAdd token as git credential? (Y/n)  n\n"}],"execution_count":4},{"cell_type":"code","source":"from pynvml import *\n\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")","metadata":{"execution":{"iopub.status.busy":"2025-03-26T03:39:44.220136Z","iopub.execute_input":"2025-03-26T03:39:44.220775Z","iopub.status.idle":"2025-03-26T03:39:44.225644Z","shell.execute_reply.started":"2025-03-26T03:39:44.220750Z","shell.execute_reply":"2025-03-26T03:39:44.224681Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"<a name='2'></a>\n#### 2. Loading dataset","metadata":{}},{"cell_type":"code","source":"# https://huggingface.co/datasets/neil-code/dialogsum-test\nhuggingface_dataset_name = \"neil-code/dialogsum-test\"\ndataset = load_dataset(huggingface_dataset_name)\ndataset\n","metadata":{"execution":{"iopub.status.busy":"2025-03-26T03:44:12.486851Z","iopub.execute_input":"2025-03-26T03:44:12.487247Z","iopub.status.idle":"2025-03-26T03:44:18.793827Z","shell.execute_reply.started":"2025-03-26T03:44:12.487219Z","shell.execute_reply":"2025-03-26T03:44:18.793132Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7651e2eaaa0461ab220b1f2833faf8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/1.81M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38efda5593854a249867a6b9a9513323"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.csv:   0%|          | 0.00/441k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3454513e10c14f40adafb26763eb1b19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/447k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8cacfd1b85f4288b31169fad8010e52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c1c97b1b2814c1c8f27d0a3eefa3eb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c46604a18f414b2885b248d3507ce859"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42a5168f62024711940b15f6fc175408"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1999\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 499\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 499\n    })\n})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"execution":{"iopub.status.busy":"2025-03-26T03:44:45.227781Z","iopub.execute_input":"2025-03-26T03:44:45.228148Z","iopub.status.idle":"2025-03-26T03:44:45.233704Z","shell.execute_reply.started":"2025-03-26T03:44:45.228117Z","shell.execute_reply":"2025-03-26T03:44:45.233035Z"},"trusted":true},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'id': 'train_0',\n 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n 'topic': 'get a check-up'}"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"<a name='3'></a>\n#### 3. Create bitsandbytes configuration\n\n","metadata":{}},{"cell_type":"code","source":"# compute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',#指定量化存储的类型normal f4，指的是程正态分布的4bit精度参数\n        bnb_4bit_compute_dtype='float16',#计算过程中使用的数据类型，影响的是4bit精度的参数在计算过程中如何计算\n        bnb_4bit_use_double_quant=False,\n    )\ndevice_map = {\"\": 0}","metadata":{"execution":{"iopub.status.busy":"2025-03-26T03:44:48.882224Z","iopub.execute_input":"2025-03-26T03:44:48.882568Z","iopub.status.idle":"2025-03-26T03:44:48.888275Z","shell.execute_reply.started":"2025-03-26T03:44:48.882540Z","shell.execute_reply":"2025-03-26T03:44:48.887594Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"<a name='4'></a>\n#### 4. Load Base Model\n","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=\"your_token\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T04:18:31.358157Z","iopub.execute_input":"2025-03-26T04:18:31.358471Z","iopub.status.idle":"2025-03-26T04:18:31.556911Z","shell.execute_reply.started":"2025-03-26T04:18:31.358445Z","shell.execute_reply":"2025-03-26T04:18:31.556072Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"model_name = \"NousResearch/Llama-2-7b-chat-hf\"\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name, \n                                                      device_map=device_map,\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:33:54.507800Z","iopub.execute_input":"2025-03-26T04:33:54.508180Z","iopub.status.idle":"2025-03-26T04:34:48.502170Z","shell.execute_reply.started":"2025-03-26T04:33:54.508153Z","shell.execute_reply":"2025-03-26T04:34:48.501506Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a356e9396eb43f0bc38954cc2e34ac4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"645ae3884fd844569381ad0747cbdf2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74a8e85a49a14b8095889a2adead0f74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20ae0f1851b647ca947852841f4dc317"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"171d5b4cca3f4b81b6a9022802af8852"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e270ab6ed149421ca65166b1d9933e57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"407fa153bfb945dc9c0f048ec98e682c"}},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"<a name='5'></a>\n#### 5. Tokenization\n","metadata":{}},{"cell_type":"code","source":"# https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\ntokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:35:53.646186Z","iopub.execute_input":"2025-03-26T04:35:53.646533Z","iopub.status.idle":"2025-03-26T04:35:56.804464Z","shell.execute_reply.started":"2025-03-26T04:35:53.646511Z","shell.execute_reply":"2025-03-26T04:35:56.803723Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22ccc6fbbd86484288d3876c420363fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43e09ad8638a403fab24d8387e5a8243"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"873606d8963a4f539ce2677118034e48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f03a128d4ff4689a6c8461eb8cdd65a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26fbb52332914684be0b7dc106160051"}},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:36:44.675700Z","iopub.execute_input":"2025-03-26T04:36:44.676028Z","iopub.status.idle":"2025-03-26T04:36:44.681057Z","shell.execute_reply.started":"2025-03-26T04:36:44.676007Z","shell.execute_reply":"2025-03-26T04:36:44.680171Z"},"trusted":true},"outputs":[{"name":"stdout","text":"GPU memory occupied: 4550 MB.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token\n\ndef gen(model,p, maxlen=100, sample=True):\n    toks = eval_tokenizer(p, return_tensors=\"pt\")\n    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:38:16.187303Z","iopub.execute_input":"2025-03-26T04:38:16.187664Z","iopub.status.idle":"2025-03-26T04:38:16.580194Z","shell.execute_reply.started":"2025-03-26T04:38:16.187642Z","shell.execute_reply":"2025-03-26T04:38:16.579494Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"<a name='6'></a>\n#### 6. Test the Model with Zero Shot Inferencing","metadata":{}},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nseed = 42\nset_seed(seed)\n\nindex = 10\nprompt = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\n# def prompt_instruct_format(prompt):\n#     output = f\"Task:Summarize the following conversation.\\nInput:\\n{prompt}\\nOutput:\\n\"\n#     return output\n# formatted_prompt = prompt_instruct_format(prompt)","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:59:08.768319Z","iopub.execute_input":"2025-03-26T04:59:08.768771Z","iopub.status.idle":"2025-03-26T04:59:08.776242Z","shell.execute_reply.started":"2025-03-26T04:59:08.768727Z","shell.execute_reply":"2025-03-26T04:59:08.775306Z"},"trusted":true},"outputs":[{"name":"stdout","text":"CPU times: user 3.41 ms, sys: 0 ns, total: 3.41 ms\nWall time: 2.56 ms\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\nres = gen(original_model,formatted_prompt,100,)\n#print(res[0])\noutput = res[0].split('Output:\\n')[1]\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:13:29.756473Z","iopub.execute_input":"2025-03-26T05:13:29.756769Z","iopub.status.idle":"2025-03-26T05:13:32.439360Z","shell.execute_reply.started":"2025-03-26T05:13:29.756748Z","shell.execute_reply":"2025-03-26T05:13:32.438401Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nPerson 1 gave Person 2 a gift and invited them to dance. Person 2 thanked them and said they were happy they remembered. Person 1 complimented their appearance and they both agreed to have a drink together to celebrate Person 2's birthday.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"<a name='7'></a>\n#### 7. Pre-processing dataset","metadata":{}},{"cell_type":"code","source":"def create_prompt_formats(sample):\n    \"\"\"\n    Format various fields of the sample ('instruction','output')\n    Then concatenate them using two newline characters \n    :param sample: Sample dictionnary\n    \"\"\"\n    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n    RESPONSE_KEY = \"### Output:\"\n    END_KEY = \"### End\"\n    \n    blurb = f\"\\n{INTRO_BLURB}\"\n    instruction = f\"{INSTRUCTION_KEY}\"\n    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n    end = f\"{END_KEY}\"\n    \n    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n\n    formatted_prompt = \"\\n\\n\".join(parts)\n    sample[\"text\"] = formatted_prompt\n\n    return sample","metadata":{"execution":{"iopub.status.busy":"2025-03-26T05:52:27.760285Z","iopub.execute_input":"2025-03-26T05:52:27.760611Z","iopub.status.idle":"2025-03-26T05:52:27.765577Z","shell.execute_reply.started":"2025-03-26T05:52:27.760585Z","shell.execute_reply":"2025-03-26T05:52:27.764741Z"},"trusted":true},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def get_max_length(model):\n    conf = model.config\n    max_length = None\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length = getattr(model.config, length_setting, None)\n        if max_length:\n            print(f\"Found max lenth: {max_length}\")\n            break\n    if not max_length:\n        max_length = 1024\n        print(f\"Using default max length: {max_length}\")\n    return max_length\n\n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )","metadata":{"execution":{"iopub.status.busy":"2025-03-26T05:53:34.163686Z","iopub.execute_input":"2025-03-26T05:53:34.164018Z","iopub.status.idle":"2025-03-26T05:53:34.168818Z","shell.execute_reply.started":"2025-03-26T05:53:34.163964Z","shell.execute_reply":"2025-03-26T05:53:34.167999Z"},"trusted":true},"outputs":[],"execution_count":39},{"cell_type":"code","source":"from functools import partial\n\ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    # Add prompt to each sample\n    print(\"Preprocessing dataset...\")\n    dataset = dataset.map(create_prompt_formats)#, batched=True)\n    \n    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n    dataset = dataset.map(\n        _preprocessing_function,\n        batched=True,\n        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n    )\n\n    # Filter out samples that have input_ids exceeding max_length\n    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n    \n    # Shuffle dataset\n    dataset = dataset.shuffle(seed=seed)\n\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2025-03-26T05:53:55.786349Z","iopub.execute_input":"2025-03-26T05:53:55.786627Z","iopub.status.idle":"2025-03-26T05:53:55.791756Z","shell.execute_reply.started":"2025-03-26T05:53:55.786607Z","shell.execute_reply":"2025-03-26T05:53:55.790660Z"},"trusted":true},"outputs":[],"execution_count":40},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2025-03-26T05:53:56.618391Z","iopub.execute_input":"2025-03-26T05:53:56.618670Z","iopub.status.idle":"2025-03-26T05:53:56.623145Z","shell.execute_reply.started":"2025-03-26T05:53:56.618651Z","shell.execute_reply":"2025-03-26T05:53:56.622173Z"},"trusted":true},"outputs":[{"name":"stdout","text":"GPU memory occupied: 4734 MB.\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# ## Pre-process dataset\nmax_length = get_max_length(original_model)\nprint(max_length)\n\ntrain_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\neval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])","metadata":{"execution":{"iopub.status.busy":"2025-03-26T05:54:07.749289Z","iopub.execute_input":"2025-03-26T05:54:07.749610Z","iopub.status.idle":"2025-03-26T05:54:13.165885Z","shell.execute_reply.started":"2025-03-26T05:54:07.749583Z","shell.execute_reply":"2025-03-26T05:54:13.165259Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Found max lenth: 4096\n4096\nPreprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1d1367c4b9548fa83da8c4b9882fac0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86470bb7b55240fe85ccfcada2308ceb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31e20e0cb56441c2a0a059eec19c99ba"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3249d9d267c4067b7860bea1d637284"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d273f865a430462197ec5511ac54360f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"823f50fe54724f299dea757d0e80b0a1"}},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"print(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {eval_dataset.shape}\")\nprint(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2025-03-26T05:57:48.619483Z","iopub.execute_input":"2025-03-26T05:57:48.619812Z","iopub.status.idle":"2025-03-26T05:57:48.626520Z","shell.execute_reply.started":"2025-03-26T05:57:48.619789Z","shell.execute_reply":"2025-03-26T05:57:48.625485Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Shapes of the datasets:\nTraining: (1999, 3)\nValidation: (499, 3)\nDataset({\n    features: ['text', 'input_ids', 'attention_mask'],\n    num_rows: 1999\n})\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"<a name='8'></a>\n#### 8. Setup the PEFT/LoRA model for Fine-Tuning","metadata":{}},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"execution":{"iopub.status.busy":"2025-03-26T06:05:46.890621Z","iopub.execute_input":"2025-03-26T06:05:46.890956Z","iopub.status.idle":"2025-03-26T06:05:46.902675Z","shell.execute_reply.started":"2025-03-26T06:05:46.890936Z","shell.execute_reply":"2025-03-26T06:05:46.901732Z"},"trusted":true},"outputs":[{"name":"stdout","text":"trainable model parameters: 16777216\nall model parameters: 3517190144\npercentage of trainable model parameters: 0.48%\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"print(original_model)","metadata":{"execution":{"iopub.status.busy":"2025-03-25T13:33:18.583565Z","iopub.execute_input":"2025-03-25T13:33:18.583920Z","iopub.status.idle":"2025-03-25T13:33:18.589984Z","shell.execute_reply.started":"2025-03-25T13:33:18.583889Z","shell.execute_reply":"2025-03-25T13:33:18.589106Z"},"trusted":true},"outputs":[{"name":"stdout","text":"PhiForCausalLM(\n  (model): PhiModel(\n    (embed_tokens): Embedding(51200, 2560)\n    (layers): ModuleList(\n      (0-31): 32 x PhiDecoderLayer(\n        (self_attn): PhiAttention(\n          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n        )\n        (mlp): PhiMLP(\n          (activation_fn): NewGELUActivation()\n          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n        )\n        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (rotary_emb): PhiRotaryEmbedding()\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nLora_config = LoraConfig(\n    r=32, #Rank\n    lora_alpha=32,\n    target_modules=[\n        'q_proj',\n        # 'k_proj',\n        'v_proj'\n        # 'dense'\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\n# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\noriginal_model.gradient_checkpointing_enable()\n\n# 2 - Using the prepare_model_for_kbit_training method from PEFT\noriginal_model = prepare_model_for_kbit_training(original_model)\n\npeft_model = get_peft_model(original_model, config)","metadata":{"execution":{"iopub.status.busy":"2025-03-26T06:03:33.298355Z","iopub.execute_input":"2025-03-26T06:03:33.298716Z","iopub.status.idle":"2025-03-26T06:03:33.540183Z","shell.execute_reply.started":"2025-03-26T06:03:33.298692Z","shell.execute_reply":"2025-03-26T06:03:33.539456Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"Once everything is set up and the base model is prepared, we can use the print_trainable_parameters() helper function to see how many trainable parameters are in the model.","metadata":{}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))","metadata":{"execution":{"iopub.status.busy":"2025-03-26T06:05:55.232355Z","iopub.execute_input":"2025-03-26T06:05:55.232640Z","iopub.status.idle":"2025-03-26T06:05:55.240009Z","shell.execute_reply.started":"2025-03-26T06:05:55.232621Z","shell.execute_reply":"2025-03-26T06:05:55.239241Z"},"trusted":true},"outputs":[{"name":"stdout","text":"trainable model parameters: 16777216\nall model parameters: 3517190144\npercentage of trainable model parameters: 0.48%\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"# See how the model looks different now, with the LoRA adapters added:\nprint(peft_model)","metadata":{"execution":{"iopub.status.busy":"2025-03-26T06:06:02.308408Z","iopub.execute_input":"2025-03-26T06:06:02.308700Z","iopub.status.idle":"2025-03-26T06:06:02.317224Z","shell.execute_reply.started":"2025-03-26T06:06:02.308680Z","shell.execute_reply":"2025-03-26T06:06:02.316375Z"},"trusted":true},"outputs":[{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"<a name='9'></a>\n#### 9. Train PEFT Adapter\n\nDefine training arguments and create Trainer instance.","metadata":{}},{"cell_type":"code","source":"output_dir = './peft-dialogue-summary-training/final-checkpoint'\nimport transformers\n\npeft_training_args = TrainingArguments(\n    output_dir = output_dir,\n    num_train_epochs=1,\n    warmup_steps=100,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=500,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=25,\n    logging_dir=\"./logs\",\n    save_strategy=\"steps\",\n    save_steps=25,\n    evaluation_strategy=\"steps\",\n    eval_steps=25,\n    do_eval=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    overwrite_output_dir = 'True',\n    group_by_length=True,\n    \n)\n\npeft_model.config.use_cache = False\n\npeft_trainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=peft_training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"execution":{"iopub.status.busy":"2025-03-26T06:56:42.099447Z","iopub.execute_input":"2025-03-26T06:56:42.099730Z","iopub.status.idle":"2025-03-26T06:56:42.135346Z","shell.execute_reply.started":"2025-03-26T06:56:42.099709Z","shell.execute_reply":"2025-03-26T06:56:42.134664Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"peft_training_args.device","metadata":{"execution":{"iopub.status.busy":"2025-03-26T06:56:45.158679Z","iopub.execute_input":"2025-03-26T06:56:45.158965Z","iopub.status.idle":"2025-03-26T06:56:45.164067Z","shell.execute_reply.started":"2025-03-26T06:56:45.158944Z","shell.execute_reply":"2025-03-26T06:56:45.163246Z"},"trusted":true},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"peft_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2025-03-26T06:56:48.107196Z","iopub.execute_input":"2025-03-26T06:56:48.107492Z","iopub.status.idle":"2025-03-26T10:05:39.494558Z","shell.execute_reply.started":"2025-03-26T06:56:48.107472Z","shell.execute_reply":"2025-03-26T10:05:39.493792Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 3:08:26, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.216400</td>\n      <td>1.186426</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.946200</td>\n      <td>1.204449</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.211600</td>\n      <td>1.189962</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.951300</td>\n      <td>1.222731</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.213200</td>\n      <td>1.187550</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.981100</td>\n      <td>1.199717</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.207400</td>\n      <td>1.179030</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.977700</td>\n      <td>1.184613</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.254700</td>\n      <td>1.173273</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.037200</td>\n      <td>1.179059</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.267400</td>\n      <td>1.172287</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.992200</td>\n      <td>1.171973</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>1.227800</td>\n      <td>1.169660</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.029400</td>\n      <td>1.170697</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>1.211800</td>\n      <td>1.166266</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.008500</td>\n      <td>1.166119</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>1.238900</td>\n      <td>1.165067</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.019500</td>\n      <td>1.163852</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>1.246800</td>\n      <td>1.164039</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.003800</td>\n      <td>1.163982</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=1.1121438522338867, metrics={'train_runtime': 11330.3093, 'train_samples_per_second': 0.177, 'train_steps_per_second': 0.044, 'total_flos': 2.5096289914773504e+16, 'train_loss': 1.1121438522338867, 'epoch': 1.0})"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2025-03-26T10:52:36.299992Z","iopub.execute_input":"2025-03-26T10:52:36.300351Z","iopub.status.idle":"2025-03-26T10:52:36.305199Z","shell.execute_reply.started":"2025-03-26T10:52:36.300310Z","shell.execute_reply":"2025-03-26T10:52:36.304471Z"},"trusted":true},"outputs":[{"name":"stdout","text":"GPU memory occupied: 10586 MB.\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"# Free memory for merging weights\ndel original_model\ndel peft_trainer\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2025-03-26T10:59:00.290507Z","iopub.execute_input":"2025-03-26T10:59:00.290821Z","iopub.status.idle":"2025-03-26T10:59:00.495728Z","shell.execute_reply.started":"2025-03-26T10:59:00.290799Z","shell.execute_reply":"2025-03-26T10:59:00.495058Z"},"trusted":true},"outputs":[],"execution_count":61},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2025-03-26T10:59:02.931720Z","iopub.execute_input":"2025-03-26T10:59:02.932085Z","iopub.status.idle":"2025-03-26T10:59:02.936742Z","shell.execute_reply.started":"2025-03-26T10:59:02.932057Z","shell.execute_reply":"2025-03-26T10:59:02.935871Z"},"trusted":true},"outputs":[{"name":"stdout","text":"GPU memory occupied: 5332 MB.\n","output_type":"stream"}],"execution_count":62},{"cell_type":"markdown","source":"<a name='10'></a>\n#### 10. Evaluate the Model Qualitatively (Human Evaluation)","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nbase_model_id = \"NousResearch/Llama-2-7b-chat-hf\"\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2025-03-26T10:59:11.923340Z","iopub.execute_input":"2025-03-26T10:59:11.923632Z","iopub.status.idle":"2025-03-26T10:59:38.406461Z","shell.execute_reply.started":"2025-03-26T10:59:11.923613Z","shell.execute_reply":"2025-03-26T10:59:38.405488Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fed62fe215a4c159cb50696b89a01b9"}},"metadata":{}}],"execution_count":63},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2025-03-26T10:59:45.315497Z","iopub.execute_input":"2025-03-26T10:59:45.315828Z","iopub.status.idle":"2025-03-26T10:59:45.737276Z","shell.execute_reply.started":"2025-03-26T10:59:45.315801Z","shell.execute_reply":"2025-03-26T10:59:45.736538Z"},"trusted":true},"outputs":[],"execution_count":64},{"cell_type":"code","source":"from peft import PeftModel\n\nft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-500\",torch_dtype=torch.float16,is_trainable=False)","metadata":{"execution":{"iopub.status.busy":"2025-03-26T10:59:53.148803Z","iopub.execute_input":"2025-03-26T10:59:53.149128Z","iopub.status.idle":"2025-03-26T10:59:53.501551Z","shell.execute_reply.started":"2025-03-26T10:59:53.149104Z","shell.execute_reply":"2025-03-26T10:59:53.500869Z"},"trusted":true},"outputs":[],"execution_count":65},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nset_seed(seed)\n\nindex = 10\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nprompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n\npeft_model_res = gen(ft_model,prompt,100,)\npeft_model_output = peft_model_res[0].split('Output:\\n')[1]\n#print(peft_model_output)\nprefix, success, result = peft_model_output.partition('#End')\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'PEFT MODEL:\\n{prefix}')","metadata":{"execution":{"iopub.status.busy":"2025-03-26T10:59:55.168706Z","iopub.execute_input":"2025-03-26T10:59:55.169025Z","iopub.status.idle":"2025-03-26T11:00:00.993714Z","shell.execute_reply.started":"2025-03-26T10:59:55.169001Z","shell.execute_reply":"2025-03-26T11:00:00.992990Z"},"trusted":true},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n---------------------------------------------------------------------------------------------------\nPEFT MODEL:\nBrian invites #Person1# to dance and #Person1# thanks him. #Person1# thinks the party is wonderful and #Person1# looks great.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday.\n\nEnd:\n\n\nCPU times: user 5.69 s, sys: 126 ms, total: 5.81 s\nWall time: 5.82 s\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"<a name='11'></a>\n#### 11. Evaluate the Model Quantitatively (with ROUGE Metric)\nPerform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time). ","metadata":{}},{"cell_type":"code","source":"original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2025-03-26T11:00:40.943661Z","iopub.execute_input":"2025-03-26T11:00:40.943969Z","iopub.status.idle":"2025-03-26T11:00:46.521916Z","shell.execute_reply.started":"2025-03-26T11:00:40.943948Z","shell.execute_reply":"2025-03-26T11:00:46.521222Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc656f136496449db6cce3cca8884a99"}},"metadata":{}}],"execution_count":67},{"cell_type":"code","source":"import pandas as pd\n\ndialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    human_baseline_text_output = human_baseline_summaries[idx]\n    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n    \n    original_model_res = gen(original_model,prompt,100,)\n    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n    \n    peft_model_res = gen(ft_model,prompt,100,)\n    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n    #print(peft_model_output)\n    peft_model_text_output, success, result = peft_model_output.partition('#End')\n    \n\n    original_model_summaries.append(original_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"execution":{"iopub.status.busy":"2025-03-26T11:01:02.238240Z","iopub.execute_input":"2025-03-26T11:01:02.238546Z","iopub.status.idle":"2025-03-26T11:02:32.416494Z","shell.execute_reply.started":"2025-03-26T11:01:02.238525Z","shell.execute_reply":"2025-03-26T11:02:32.415577Z"},"trusted":true},"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"                            human_baseline_summaries  \\\n0  Ms. Dawson helps #Person1# to write a memo to ...   \n1  In order to prevent employees from wasting tim...   \n2  Ms. Dawson takes a dictation for #Person1# abo...   \n3  #Person2# arrives late because of traffic jam....   \n4  #Person2# decides to follow #Person1#'s sugges...   \n5  #Person2# complains to #Person1# about the tra...   \n6  #Person1# tells Kate that Masha and Hero get d...   \n7  #Person1# tells Kate that Masha and Hero are g...   \n8  #Person1# and Kate talk about the divorce betw...   \n9  #Person1# and Brian are at the birthday party ...   \n\n                            original_model_summaries  \\\n0  Ms. Dawson, I need you to take a dictation for...   \n1  Ms. Dawson, I need you to take a dictation for...   \n2  Ms. Dawson, I need you to take a dictation for...   \n3  Person 1: You're finally here! What took so lo...   \n4  Person 1: You're finally here! What took so lo...   \n5  Person 1: You're finally here! What took so lo...   \n6  Masha and Hero are getting divorced. Kate said...   \n7  Masha and Hero are getting divorced. Kate said...   \n8  Masha and Hero are getting divorced. Kate said...   \n9  Person 1 gave Person 2 a gift and invited them...   \n\n                                peft_model_summaries  \n0  #Person1# tells #Person2# to take a dictation ...  \n1  #Person1# needs Ms. Dawson to take a dictation...  \n2  #Person1# needs Ms. Dawson to take a dictation...  \n3  #Person1# suggests #Person2# take the public t...  \n4  #Person2# is stuck in traffic again. #Person1#...  \n5  #Person1# suggests #Person2# take the subway t...  \n6  Masha and Hero are getting divorced. Masha and...  \n7  Masha and Hero are getting divorced. Masha and...  \n8  Masha and Hero are getting divorced. Masha and...  \n9  #Person1# gives Brian a birthday present and i...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>peft_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n      <td>Ms. Dawson, I need you to take a dictation for...</td>\n      <td>#Person1# tells #Person2# to take a dictation ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In order to prevent employees from wasting tim...</td>\n      <td>Ms. Dawson, I need you to take a dictation for...</td>\n      <td>#Person1# needs Ms. Dawson to take a dictation...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n      <td>Ms. Dawson, I need you to take a dictation for...</td>\n      <td>#Person1# needs Ms. Dawson to take a dictation...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person2# arrives late because of traffic jam....</td>\n      <td>Person 1: You're finally here! What took so lo...</td>\n      <td>#Person1# suggests #Person2# take the public t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n      <td>Person 1: You're finally here! What took so lo...</td>\n      <td>#Person2# is stuck in traffic again. #Person1#...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Person2# complains to #Person1# about the tra...</td>\n      <td>Person 1: You're finally here! What took so lo...</td>\n      <td>#Person1# suggests #Person2# take the subway t...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n      <td>Masha and Hero are getting divorced. Kate said...</td>\n      <td>Masha and Hero are getting divorced. Masha and...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n      <td>Masha and Hero are getting divorced. Kate said...</td>\n      <td>Masha and Hero are getting divorced. Masha and...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>#Person1# and Kate talk about the divorce betw...</td>\n      <td>Masha and Hero are getting divorced. Kate said...</td>\n      <td>Masha and Hero are getting divorced. Masha and...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>#Person1# and Brian are at the birthday party ...</td>\n      <td>Person 1 gave Person 2 a gift and invited them...</td>\n      <td>#Person1# gives Brian a birthday present and i...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":68},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2025-03-26T11:03:13.603911Z","iopub.execute_input":"2025-03-26T11:03:13.604236Z","iopub.status.idle":"2025-03-26T11:03:20.004832Z","shell.execute_reply.started":"2025-03-26T11:03:13.604213Z","shell.execute_reply":"2025-03-26T11:03:20.003930Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=bf0141312004209687394f99fcab2298f590cfdd601f2214763329423391cd4b\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"import evaluate\n\nrouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"execution":{"iopub.status.busy":"2025-03-26T11:03:43.013716Z","iopub.execute_input":"2025-03-26T11:03:43.014098Z","iopub.status.idle":"2025-03-26T11:03:45.184561Z","shell.execute_reply.started":"2025-03-26T11:03:43.014067Z","shell.execute_reply":"2025-03-26T11:03:45.183625Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a9db534cd844f7e98dbfad5e4aa16f4"}},"metadata":{}},{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.24553039323884007, 'rouge2': 0.1007362847307088, 'rougeL': 0.18284898441649494, 'rougeLsum': 0.19945703822890523}\nPEFT MODEL:\n{'rouge1': 0.2814897095344895, 'rouge2': 0.09942264114903879, 'rougeL': 0.22337456515019954, 'rougeLsum': 0.2415772560391144}\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2025-03-26T11:04:03.480947Z","iopub.execute_input":"2025-03-26T11:04:03.481314Z","iopub.status.idle":"2025-03-26T11:04:03.487028Z","shell.execute_reply.started":"2025-03-26T11:04:03.481285Z","shell.execute_reply":"2025-03-26T11:04:03.486226Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\nrouge1: 3.60%\nrouge2: -0.13%\nrougeL: 4.05%\nrougeLsum: 4.21%\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}